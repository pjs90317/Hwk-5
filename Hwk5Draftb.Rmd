---
title: "Homework 5"
author: Patrick Sinclair, Kieran Yuen
output: github_document
---

```{r include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(stargazer)
load("acs2017_ny_data.RData")
```
```{r}
attach(acs2017_ny)
healthcare <- (IND == "7970" | IND == "7980" | IND == "7990" | IND == "8070" | IND == "8080" | IND == "8090" | IND == "8170" | IND == "8180" | IND == "8190" | IND == "8270")
use_varb <- (AGE >= 25) & (AGE <= 70) & (LABFORCE == 2) & (WKSWORK2 > 4) & (UHRSWORK >= 35) & (female == 1) & ((educ_college == 1) | (educ_advdeg == 1)) & healthcare
dat_meda <- subset(acs2017_ny, use_varb)
dat_med <- subset(dat_meda, select = -(veteran))
detach()
```
```{r include=FALSE}
library(AER)
```
```{r eval=FALSE}
attach(dat_med)
summary(dat_med)
detach()
```
For our subset, we decided to examine how different variables impact wages for people who identify as female working in the various aspects of the medical industry. We did this by creating an object that houses different industry codes pertaining to the medical field, including physician and dentist offices, hospitals, nursing facilities and other health-care practitioners.
```{r eval=FALSE}
# creating different objects for work departure time
# want to create different objects within industry to subset office workers, hospitals, nursing facilities etc
attach(dat_med)
DEPARTS
length(DEPARTS)
graveyard <- ((DEPARTS >= 0) & (DEPARTS <= 459))
morning <- ((DEPARTS >= 500) & (DEPARTS <=930))
daytime <- ((DEPARTS >= 931) & (DEPARTS <=1700))
evening <- ((DEPARTS >= 1701) & (DEPARTS <=2359))
summary(graveyard)
summary(morning)
summary(daytime)
summary(evening)
table(morning)
deptime <- factor((1*evening + 2*morning + 3*daytime + 4*graveyard), levels = c(1, 2, 3, 4), labels = c( "Night", "Grave", "Morn", "Mid"))
table(deptime)
sum(morning+daytime)
detach()
```
```{r eval=FALSE}
attach(dat_med)
summary(INCWAGE)
detach()
```
For our initial regression, we elected to use the [INCWAGE](https://cps.ipums.org/cps-action/variables/INCWAGE#description_section) observations as our measure of wage. We made this decision as we believe it is reasonable to assume that most of the data observations in our subset (females in the medical field with a Bachelor or Advanced degree) would only hold one job. INCWAGE is 'the total pre-tax and salary income' each of the respondent's earned as an employee in the previous 12 months prior to the survey.

```{r}
attach(dat_med)
lma <- lm((INCWAGE ~ AGE + I(AGE^2) + educ_college))
# summary(lma)
# lmdep <- lm((INCWAGE ~ AGE + I(AGE^2) + educ_college + deptime))
# summary(lmdep)
suppressWarnings(stargazer(lma, type = "text"))
# par(mfrow = c(2, 2))
# plot(lma)
detach()
```
The regressor coefficients from the initial regression all appear to be statistically significant, the only exception being the intercept coefficient. What is curious is that holding a Bachelor's degree has a *negative* correlation with wages within the subset, as does $Age^2$. However, for our observations this makes sense; we limited our observations to people who hold Bachelor's or Advanced degrees. From an outsider's perspective of the industry, those whole hold advanced degrees or extra certifications within the medical field would be those who have specialized in particular fields, commanding higher wages for their specialized knowledge. A Bachelor's degree is closer to an entry level requirement in the field, so the comparative wages are significantly lower.

From this regression, we created our prediction model using 50% of the 2626 observations in our subset in order to give the model a sizable enough sample from which to draw its comparisons. For easy visualization, the jitter has been set to 0.1, to demonstrate the trends at different ages and the range of the INCWAGE axis has been capped at 300,000.

To draw some comparison from the linear regression, we set the prediction model to predict wages for those with advanced degrees.  
```{r}
attach(dat_med)
NNobs <- length(INCWAGE)
set.seed(12345)
graph_obs <- (runif(NNobs) < 0.5)
med_graph <-subset(dat_med, graph_obs)
```
```{r}
png(filename="~/CCNY/Econometrics/Homework/Draft Hwk Files/Homework-5/Hwk-5/initlmplot.png")
plot(INCWAGE ~ jitter(AGE, factor = 0.1), pch = 16, col = rgb(0, 0.5, 0, alpha = 0.2), ylim = c(0,300000), data = med_graph)
medpredict <- data.frame(AGE = 25:70, female = 1, educ_college = 1, educ_advdeg = 1)
dev.off()
```
```{r}
plot(INCWAGE ~ jitter(AGE, factor = 0.1), pch = 16, col = rgb(0, 0.5, 0, alpha = 0.2), ylim = c(0,300000), data = med_graph)
medpredict <- data.frame(AGE = 25:70, female = 1, educ_college = 1, educ_advdeg = 1)
# medpredict$yhat <- predict(lma, newdata = medpredict)
# lines(yhat ~ AGE, data = medpredict)
# max(medpredict$yhat) # peak predicted wage
medpredictb <- data.frame(AGE = 25:70, female = 1, educ_college = 0, educ_advdeg = 1)
medpredictb$yhat <- predict(lma, newdata = medpredictb)
lines(yhat ~ AGE, data = medpredictb)
# plot(INCWAGE ~ jitter(AGE, factor = 0.1), pch = 16, col = rgb(0.5, 0.5, 0.5, alpha = 0.2), ylim = c(0,300000), data = med_graph)
# medpredictb <- data.frame(AGE = 25:70, female = 1, educ_college = 0, educ_advdeg = 1)
# medpredictb$yhat <- predict(lma, newdata = medpredictb)
# lines(yhat ~ AGE, data = medpredictb, col = "blue")
# max(medpredictb$yhat) # peak predicted wage
detach()
```
The plot for the predicted values shows us a gently sloped concave curve that has a peak predicted value of
```{r}
max(medpredictb$yhat)
detach()
```
When we added high polynomials, $Age^3$, $Age^4$ and $Age^5$, to the regression, the absolute values of the corresponding coefficients get progressively smaller.
```{r}
# sets up lm with polynomials included
attach(dat_med)
lmpolys <- lm((INCWAGE ~ AGE + I(AGE^2) + I(AGE^3) + I(AGE^4) + I(AGE^5) + educ_college))
# summary(lma)
# plot(lmpolys)
suppressWarnings(stargazer(lmpolys, type = "text"))
```
```{r}
# plots line on plot - peak wages AREN'T particularly different from those of the Aged^2 lm
png(filename="~/CCNY/Econometrics/Homework/Draft Hwk Files/Homework-5/Hwk-5/polyplot.png")
plot(INCWAGE ~ jitter(AGE, factor = 0.1), pch = 16, col = rgb(0, 0.5, 0, alpha = 0.2), ylim = c(0,300000), data = med_graph)
medpredictb$yhatpolys <- predict(lmpolys, newdata = medpredictb)
lines(yhatpolys ~ AGE, data = medpredictb, col = "Purple" )
lines(yhat ~ AGE, data = medpredictb, lty = 2)
legend("topleft", c("Polynomials", "Squared"), col = c("Purple", "Black"), lty = c(1,2), bty = "n")
dev.off()
```
```{r}
plot(INCWAGE ~ jitter(AGE, factor = 0.1), pch = 16, col = rgb(0, 0.5, 0, alpha = 0.2), ylim = c(0,300000), data = med_graph)
medpredictb$yhatpolys <- predict(lmpolys, newdata = medpredictb)
lines(yhatpolys ~ AGE, data = medpredictb, col = "Purple" )
lines(yhat ~ AGE, data = medpredictb, lty = 2)
legend("topleft", c("Polynomials", "Squared"), col = c("Purple", "Black"), lty = c(1,2), bty = "n")
```  
When plotted, the curves follow a similar slope and shape. Adding the extra polynomials increases the maximum predicted wage slightly to
```{r}
max(medpredictb$yhatpolys)
```
but we notice a steepening of the "Polynomials" curve as age approaches 70.
```{r}
# Hypothesis test of higher order polynomials - jointly significant
coeftest(lmpolys,vcovHC)
require(car)
# or
linearHypothesis(lmpolys, "I(AGE^2) + I(AGE^3) + I(AGE^4) + I(AGE^5) = 0", test="F")
```

Do a hypothesis test of whether all of those higher-order polynomial terms are jointly significant. Describe the pattern of predicted wage as a function of age. 

```{r}
# The pattern of predicted wage trends upward until age reaches the mid 50s, plateaus until approximately 57 years old then trends down towards 70 years old.
```

What if you used $log(Age)$? (And why would polynomials in $log(Age)$ be useless? Experiment.)

```{r}
lmlog <- lm((INCWAGE ~ AGE + log(AGE) + educ_college))
# plot(lmlog)
plot(INCWAGE ~ jitter(AGE, factor = 1), pch = 16, col = rgb(0.5, 0.5, 0.5, alpha = 0.2), ylim = c(0,300000), data = med_graph)
medpredict$yhatlog <- predict(lmlog, newdata = medpredict)
lines(yhatlog ~ AGE, data = medpredict)
max(medpredict) # peak predicted wage
# using log(AGE) smooths the curve towards the upper end of the age range. We shouldn't use polynomials as they will undo the impact of the log function.
```

Recall about how dummy variables work. If you added educ_hs in a regression using the subset given above, what would that do? (Experiment, if you aren't sure.) What is interpretation of coefficient on *educ_college* in that subset? What would happen if you put both *educ_college* and *educ_advdeg* into a regression? Are your other dummy variables in the regression working sensibly with your selection criteria?

```{r}
# Adding educ_hs in our particular subset will render it as omitted (we haven't included it as a variable in the subset). If we add the educ_advdeg variable, it will also be rendered as omitted, so R can run a comparison.
lmedu <- lm((INCWAGE ~ AGE + I(AGE^2) + educ_college + educ_advdeg))
lmedu
lmedua <- lm((INCWAGE ~ AGE + I(AGE^2) + educ_advdeg + educ_college))
lmedua
# if we change the position of the educ_advdeg variable to place it BEFORE the educ_college variable, R drops the LAST variable from the regression. What is interesting is that whichever variable is included, the coefficient is the absolute same value but has a negative value when educ_college is the included variable.
```
Why don't we use polynomial terms of dummy variables? Experiment.

```{r}
# We don't use polynomials of dummy variables as they have no effect on the regression - 1^x = 1; 0^x = 0.
```
What is the predicted wage, from your model, for a few relevant cases? Do those seem reasonable?
```{r}
# check wage as a function of age for lma and medpredict
# we can also change the medpredict objects to use the poly regression or the log regression
medpredictc <- data.frame(AGE = 25:70, female = 1, educ_college = 1, educ_advdeg = 0)
medpredictc$yhat <- predict(lma, newdata = medpredictc)
medpredict
medpredictb
medpredictc
```

What is difference in regression from using log wage as the dependent variable? Compare the pattern of predicted values from the two models (remember to take exp() of the predicted value, where the dependent is log wage). Discuss.

```{r}
# included all 3 regressions into medpredict in prior chunks to allow for easy comparison in the table
medpredict
mean(medpredict$yhat)
mean(medpredict$yhatlog)
```

Try some interactions, like this,
```{r}
# interactions
lminter <- lm(INCWAGE ~ AGE + I(AGE^2) + educ_college + I(educ_college*AGE) + I(educ_college*(AGE^2)))
lminter
lminterb <- lm(INCWAGE ~ AGE + I(AGE^2) + educ_college + I(educ_college*AGE) + educ_advdeg + I(educ_advdeg*AGE))
lminterb
```
and explain those outputs (different peaks for different groups).

What are the other variables you are using in your regression? Do they have the expected signs and patterns of significance? Explain if there is a plausible causal link from X variables to Y and not the reverse. Explain your results, giving details about the estimation, some predicted values, and providing any relevant graphics. Impress.

### Bibliography
https://cps.ipums.org/cps-action/variables/INCWAGE#description_section